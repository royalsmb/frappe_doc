"""ðŸ¤– Agentic RAG Agent - Your AI Knowledge Assistant!

This advanced example shows how to build a sophisticated RAG (Retrieval Augmented Generation) system that
leverages vector search and LLMs to provide deep insights from any knowledge base.

The agent can:
- Process and understand documents from multiple sources (PDFs, websites, text files)
- Build a searchable knowledge base using vector embeddings
- Maintain conversation context and memory across sessions
- Provide relevant citations and sources for its responses
- Generate summaries and extract key insights
- Answer follow-up questions and clarifications

Example queries to try:
- "What are the key points from this document?"
- "Can you summarize the main arguments and supporting evidence?"
- "What are the important statistics and findings?"
- "How does this relate to [topic X]?"
- "What are the limitations or gaps in this analysis?"
- "Can you explain [concept X] in more detail?"
- "What other sources support or contradict these claims?"

The agent uses:
- Vector similarity search for relevant document retrieval
- Conversation memory for contextual responses
- Citation tracking for source attribution
- Dynamic knowledge base updates

View the README for instructions on how to run the application.
"""

from typing import Optional

from agno.agent import Agent, AgentMemory, RunResponse
from agno.embedder.openai import OpenAIEmbedder
from agno.knowledge import AgentKnowledge
from agno.memory.db.postgres import PgMemoryDb
from agno.models.anthropic import Claude
from agno.models.google import Gemini
from agno.models.openai import OpenAIChat
from agno.storage.agent.postgres import PostgresAgentStorage
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.vectordb.pgvector import PgVector
import frappe
from agno.embedder.google import GeminiEmbedder
from agno.knowledge.pdf_url import PDFUrlKnowledgeBase
from agno.embedder.fastembed import FastEmbedEmbedder

import os
config = frappe.get_site_config()

os.environ['GOOGLE_API_KEY'] = config.get('gemini_api_key')
os.environ['OPENAI_API_KEY'] = config.get('openai_api_key')
os.environ['GROQ_API_KEY'] = config.get('groq_api_key')
db_url = config.get("pg_url")

model_id = "openai:gpt-4o",
user_id: Optional[str] = "utgbuddy",
session_id: Optional[str] = "utgbuddy",
debug_mode: bool = True,
"""Get an Agentic RAG Agent with Memory."""
# Parse model provider and name
provider = "openai"
model_name = "gpt-4o"

# Select appropriate model class based on provider
if provider == "openai":
    model = OpenAIChat(id=model_name)
elif provider == "google":
    model = Gemini(id=model_name)
elif provider == "anthropic":
    model = Claude(id=model_name)
else:
    raise ValueError(f"Unsupported model provider: {provider}")
# Define persistent memory for chat history
memory = AgentMemory(
    db=PgMemoryDb(
        table_name="gamswitch", db_url=db_url
    ),  # Persist memory in Postgres
    create_user_memories=True,  # Store user preferences
    create_session_summary=True,  # Store conversation summaries
)
@frappe.whitelist(allow_guest=True)
def agent(table_name=None, name=None):
    knowledge_base = PDFUrlKnowledgeBase(
        urls = ['https://test.royalsmb.com/files/MPGS%20Agreement%20Template%20-Clean.pdf'],
        vector_db=PgVector(
            db_url=db_url,
            table_name= "fastembed",
            schema="ai",
            embedder=FastEmbedEmbedder(),
        ),
        num_documents=3,  # Retrieve 3 most relevant documents
    )
    # knowledge_base.load(recreate=False, upsert=True)
    # Create the Agent
    agent =  Agent(
        name="fastembed",
        session_id="kjdsjwdu9382okwq",  # Track session ID for persistent conversations
        user_id="3611704",
        model=Gemini(id="gemini-2.0-flash-exp"),
        storage=PostgresAgentStorage(
            table_name="fastembed", 
            db_url=db_url
        ),  # Persist session data
        memory=memory,  # Add memory to the agent
        knowledge=knowledge_base,  # Add knowledge base
        # description="You are a friendly Study Assistant AI that helps students master their course material in the best way possible from the knowledbase",
        # instructions=[
        #     " Extract and combine only the main educational content. IGNORE ALL of the following:",  
        #     " - Author names, titles, or credentials",  
        #     " - Table of contents",  
        #     " - Headers or footers",  
        #     " - Page numbers",  
        #     " - Presentation metadata",  
        #     " - Section numbers or labels",  
        #     " Combine the remaining content into a single knowledge base and divide it into logical sections. The less sections the better. Minimum 2 sections are required. Maximum 7 sections are allowed. Each section should be a self-contained unit focusing on a distinct topic or subtopic.",  
        #     " Please divide this content into logical sections. For each section, provide:",  
        #     " 1. A title (2-5 words)",  
        #     " 2. The full content of that section",  
        #     " IMPORTANT: You MUST follow this EXACT format:",  
        #     " {\"title\": \"Section Title\", \"content\": \"Full section content\"}",  
        #     "",  
        #     " OUTPUT FORMAT RULES:",  
        #     " 1. Each line MUST be a complete JSON object with ONLY \"title\" and \"content\" fields",  
        #     " 2. Output one complete JSON object per line",  
        #     " 3. Do not use markdown code blocks",  
        #     " 4. Do not add any text before or after the JSON",  
        #     " 5. Do not split JSON objects across multiple lines",  
        #     " 6. Each line MUST look EXACTLY like this: {\"title\":\"Section Title\",\"content\":\"Full section content\"}",  
        #     " 7. DO NOT include any other fields in the JSON object",  
        #     "",  
        #     " CONTENT CHUNKING RULES:",  
        #     " 1. Each chunk should be a logical, self-contained unit of study",  
        #     " 2. Do not rely on how the content was split; instead, just split based on logical boundaries",  
        #     " 3. Include full context in each chunk",  
        #     " 4. Maintain proper sequence and flow between chunks",  
        #     " 5. Use clear, descriptive titles that reflect the content",  
        #     " 6. Ensure no critical information is lost in the splitting",  
            
        # ],
        
    

        search_knowledge=True,  # This setting gives the model a tool to search the knowledge base for information
        read_chat_history=True,  # This setting gives the model a tool to get chat history
        # tools=[DuckDuckGoTools()],
        markdown=True,  # This setting tellss the model to format messages in markdown
        show_tool_calls=True,
        add_history_to_messages=True,  # Adds chat history to messages
        add_datetime_to_instructions=True,
        debug_mode=debug_mode,
        read_tool_call_history=True,
        num_history_responses=3,
    )
    
    
    response: RunResponse = agent.run("How Much gamswitch charges", markdown=True)
    content =  response.messages
    #     Access the message array.
    # Find the last entry where the role is "assistant".
    # Extract the content from that entry.
    # Return the content.
    return content[-1].content

@frappe.whitelist(allow_guest=True)
def flash_card_agent():     
    
    prompt = """Create a set of flashcards for the key concepts from this Gamswitch study note. Include the following details:

            - Term or concept
            - Definition or explanation
            - Example or application
            - Mnemonic or memory aid
            """
    agentic_rag_agent = agent("gamswitch", "utgbuddy")
    response: RunResponse = agentic_rag_agent.run(prompt, markdown=True)
    content =  response.messages
    return content[-1].content

